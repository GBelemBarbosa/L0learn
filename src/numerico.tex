%LTeX: language=pt-BR
\section{Experimentos numéricos}
Nessa seção serão efetuados experimentos com o intuito de comparar os algoritmos apresentados. 

\subsection{\textit{Setup} experimental}
\subsubsection{Geração de dados}
Assim como em \cite{fastselect}, utilizamos conjuntos de dados sintéticos para uma ampla variedade de tamanhos e configurações de problemas. A matriz de dados é gerada com distribuição gaussiana multivariada $A_{n \times p} \sim \text{MVN}(0, \Sigma)$, e então suas colunas são normalizadas tal que $\|A_i\|_2=1\ \forall i$ para simplificar as atualizações dos algoritmos. Como verdade base, é utilizado um vetor de coeficientes esparsos $x^\dagger\in\R^p$ com $k^\dagger$ entradas não nulas igualmente espaçadas e com valor $1$. É escolhido também um valor aproximado para o SNR, definido como 
\[\text{SNR} \coloneq \frac{\text{E}[\|Ax^\dagger\|_2^2]}{\text{E}[\|b-Ax^\dagger\|_2^2]},\]
e com base nessa escolha é tomado 
\[\sigma = \sqrt{\frac{\|Ax^\dagger\|_2^2}{n\cdot\text{SNR}}}.\]
Em seguida, o vetor de resposta é gerado como $b = Ax^\dagger + \epsilon$, onde $\epsilon_i \stackrel{\text{iid}}{\sim} N(0, \sigma^2)$ é independente de $A$. Dessa forma, garantimos que 
\[\frac{\|Ax^\dagger\|_2^2}{\|b-Ax^\dagger\|_2^2} = \frac{\|Ax^\dagger\|_2^2}{\|\epsilon\|_2^2}\approx \frac{\|Ax^\dagger\|_2^2}{n\sigma^2}= \text{SNR},\]
ou seja, o SNR verdadeiro seja próximo do SNR desejado.   

Foram consideradas as seguintes instâncias de $\Sigma \coloneq ((\sigma_{ij}))$:
\begin{itemize}
  \item \textit{Correlação constante}: É definido $\sigma_{ij} = \rho\ \forall i \neq j$ e $\sigma_{ii} = 1\ \forall i$;
  \item \textit{Correlação exponencial}: É definido $\sigma_{ij} = \rho^{|i-j|}\ \forall i, j$, com a convenção $0^0 = 1$.
\end{itemize}

\subsubsection{Parâmetros dos algoritmos}
Após experimentos iniciais de ajuste de hiperparâmetros, foram escolhidos $m=15,\ \delta=0.01$ e $\tau=0.25$ para o NPG. As salvaguardas $\gamma_{min}$ e $\gamma_{max}$ são usualmente tomadas como valores arbitrariamente pequenos e grandes, respectivamente. Os tamanhos de passo espectral iniciais, $\gamma^{BBR}_{0,+}$ e $\gamma^{BBR2}_{0,+}$, foram tomados via \eqref{BBR} e \eqref{BBR2} com a convenção de $x^{-1}=x^0-10^{-5}\nabla f(x^0)$. Já a métrica diagonal espectral inicial, $U_{0,+}^{DBBR}$, foi definida com esses tamanhos de passo e a convenção de $U_{-1,+}^{DBBR}$ nula. Para o VMNSPG, $\mu=10^{-3}$ apresentou os melhores resultados dentre os valores testados. Quanto ao PGCCD, somente foram ordenadas um quarto das coordenadas.



\subsection{Comparação entre PGCCD e NPG}
Consideramos a eficiência dos algoritmos propostos para o problema \eqref{R} com $\lambda_q=0$ (problema $\ell_0$ puro). Foi utilizado um conjunto de dados com correlação exponencial, $\rho=0.5,\ n=500,\ p=2000,\ k^\dagger=100,\ \text{SNR}=10$. Foi tomado $\lambda_0=0.5\frac{\norm{A^Tb}^2_\infty}{2L_f}$ para obter uma penalização razoável (veja \cite[Exemplo 3]{manuscrito} para uma explicação dessa escolha). Foram geradas 50 inicializações com suporte de tamanho $k^\dagger$ com índices uniformemente escolhidos de $[p]$ e valores com distribuição uniforme em $(0,1)$. Para cada inicialização é rodado cada algoritmo. A medida de recuperação do suporte é definida como 
\[\frac{| Supp(x) \cap Supp(x^\dagger) |}{\max \{ |Supp(x)|, k^\dagger \}},\]
que quantifica a fração de variáveis verdadeiramente ativas recuperadas, normalizada pelo máximo entre o tamanho do suporte estimado e o verdadeiro $k^\dagger$. Essa medida penaliza tanto falsos positivos quanto falsos negativos e valores próximos de 1 indicam recuperação alta. O critério de parada utilizada em todos os algoritmos foi uma mudança relativa no objetivo menor que $10^{-7}$.

A \autoref{pK} contém o \textit{box plot} do número de iterações, em que uma passagem pelas $p$ coordenadas corresponde a uma iteração do PGCCD. Essa métrica não reflete precisamente a velocidade de cada algoritmo uma vez que não considera, por exemplo, o número de sub iterações na busca linear do NPG e a reordenação efetuada no PGCCD. Mesmo assim, é possível ter uma ideia de que a convergência do PGCCD é significativamente mais rápida que a dos métodos proximais não monótonos apresentados. O VMSPG demonstra convergência mais rápida entre os métodos NPG, provavelmente devido à sua incorporação de mais informação de segunda ordem. Seu número de iterações, com mediana em torno de 30, fica o mais próximo do PGCCD, com mediana em torno de 20. O passo espectral híbrido do NSPGH parece acelerar levemente a convergência em comparação com o passo primal usado no NSPG, e ambos performam cerca de duas a três vezes pior que o PGCCD.

Contudo, uma análise da qualidade das soluções obtidas através da \autoref{pF} e \autoref{pSUP} demonstra que, apesar de mais lentos, ambos o NSPG e NSPGH obtém melhores resultados em valor objetivo e recuperação de suporte. Em particular nesse segundo aspecto, a diferença é bastante significativa. O VMNSPG obteve a pior desempenho em ambos os quesitos, apesar da sua convergência mais rápida.

A capacidade do NPG com estratégias de passo espectral encontrar soluções de qualidade está diretamente ligada a não monotonia da sequência de iterados e a permissão de tamanhos de passo maiores que permitem maior liberdade de movimento \cite{manuscrito}. No caso do VMNSPG, é possível que uma convergência local mais rápida restrinja essa movimentação e a busca por soluções melhores. A excelente recuperação do suporte indica que o NSPG e NSPGH produzem candidatos mais adequados para a aplicação da otimização combinatorial local, como será visto nos experimentos a seguir. Esse resultado é curioso visto que esses algoritmos só possuem garantia de convergência a pontos M-estacionários, enquanto o PGCCD busca mínimos CW, cuja definição é mais restritiva (veja \eqref{hier}).

Para melhor analisar as características dos limites das sequências dos algoritmos NPG, também foi executado o PGCCD a partir desses pontos para determinar se eles também eram mínimos CW. Consideramos que caso o suporte não seja alterado durante essa passagem pelo PGCCD e o número de iterações seja pequeno ($2$ ou menos) antes do critério de parada, então o ponto pode razoavelmente ser considerado (numericamente) CW. Para o NSPG, $60\%$ das inicializações resultaram em pontos CW, enquanto essa porcentagem foi $44\%$ para o NSPGH. O VMNSPG não resultou em nenhum ponto CW seguindo essa definição. 

\begin{figure}[H]
  \centering
  \subbottom[Valor objetivo alcançado.]{\label{pF}%
    \includegraphics[width=0.45\linewidth]{\boxplotPath exp-0.5-500-2000-10-100/pF-exp-0.5-500-2000-10-100.png}}
  \subbottom[Medida de recuperação do suporte.]{\label{pSUP}%
    \includegraphics[width=0.45\linewidth]{\boxplotPath exp-0.5-500-2000-10-100/pSUP-exp-0.5-500-2000-10-100.png}}\\
  \subbottom[Número de iterações (cada passagem pelas $p$ coordenadas contam como uma iteração do PGCCD)]{\label{pK}%
    \includegraphics[width=0.45\linewidth]{\boxplotPath exp-0.5-500-2000-10-100/pK-exp-0.5-500-2000-10-100.png}}
  \caption{\textit{Box plots} dos resultados dos algoritmos testados para 50 inicializações. Correlação exponencial, $\rho=0.5,\ n=500,\ p=2000,\ k^\dagger=100,\ \text{SNR}=10$.}
\end{figure}


\subsection{Comparação com otimização combinatorial}
Nesse experimento, visamos identificar o efeito que cada algoritmo tem quando empregado como \textit{inner solver} do algoritmo CPSI(1) (\autoref{alg:cpsi}). Serão testados o PGCCD, o NSPG e o NSPG seguido do PGCCD. Como comparação da qualidade das soluções, também foi incluído o NSPG sem estratégia combinatorial.

O setup experimental é idêntico ao da seção anterior. Além das medidas apresentadas ali, incluímos o número de iterações externas (iterações do \autoref{alg:cpsi}). O número de iterações total se refere ao somatório de iterações de todas as chamadas do \textit{inner solver} durante a execução. 

A \autoref{pK2} mostra que a diferença de iterações entre o PGCCD e o NSPG se tornou menos pronunciada que na \autoref{pK}. Isso se deve ao fato de que o PGCCD executa mais iterações externas em média, ou seja, precisa de mais perturbações para melhorar suas soluções, como visto na \autoref{pKout2}. A otimização combinatorial causa um grande aumento no número de iterações do NSPG.

A qualidade das soluções obtidas por métodos NSPG (incluindo sem otimização combinatorial) continua bastante superior àquelas do PGCCD nos critérios analisados (\autoref{pF2} e \autoref{pSUP2}). De fato, a otimização combinatorial melhora as soluções do NSPG, mas o ganho é relativamente pequeno nesse experimento. 

A utilização conjunta do NSPG e PGCCD se mostrou vantajosa em todos os aspectos se comparado com o NSPG puro.

\begin{figure}[H]
  \centering
  \subbottom[Valor objetivo alcançado.]{\label{pF2}%
    \includegraphics[width=0.45\linewidth]{\boxplotPath exp-0.5-500-2000-10-100/pF-exp-0.5-500-2000-10-100.png}}
  \subbottom[Medida de recuperação do suporte.]{\label{pSUP2}%
    \includegraphics[width=0.45\linewidth]{\boxplotPath exp-0.5-500-2000-10-100/pSUP-exp-0.5-500-2000-10-100.png}}\\
  \subbottom[Número de iterações (cada passagem pelas $p$ coordenadas contam como uma iteração do CD)]{\label{pK2}%
    \includegraphics[width=0.45\linewidth]{\boxplotPath exp-0.5-500-2000-10-100/pK-exp-0.5-500-2000-10-100.png}}
  \subbottom[Número de iterações externas (iterações do Algoritmo \ref{alg:cpsi})]{\label{pKout2}%
    \includegraphics[width=0.45\linewidth]{\boxplotPath exp-0.5-500-2000-10-100/pKout-exp-0.5-500-2000-10-100.png}}
  \caption{\textit{Box plots} dos resultados dos algoritmos testados para 50 inicializações. Correlação exponencial, $\rho=0.5,\ n=500,\ p=2000,\ k^\dagger=100,\ \text{SNR}=10$.}
\end{figure}

Aumentando o SNR para 300 para permitir recuperação completa do suporte, vemos uma diferença drástica na qualidade das soluções, com somente um aumento pequeno no número de iterações (\autoref{p3}). A estratégia NSPG+PGCCD continua sendo a mais atrativa, com menos soluções \textit{outliers} com um custo levemente superior em iterações, porém todas as estratégias NSPG apresentam recuperação perfeita da solução verdadeira com probabilidade alta.

\begin{figure}[H]
  \centering
  \subbottom[Valor objetivo alcançado.]{%
    \includegraphics[width=0.45\linewidth]{\boxplotPath exp-0.5-500-2000-300-100/pF-exp-0.5-500-2000-300-100.png}}
  \subbottom[Medida de recuperação do suporte.]{%
    \includegraphics[width=0.45\linewidth]{\boxplotPath exp-0.5-500-2000-300-100/pSUP-exp-0.5-500-2000-300-100.png}}\\
  \subbottom[Número de iterações (cada passagem pelas $p$ coordenadas contam como uma iteração do CD)]{%
    \includegraphics[width=0.45\linewidth]{\boxplotPath exp-0.5-500-2000-300-100/pK-exp-0.5-500-2000-300-100.png}}
  \subbottom[Número de iterações externas (iterações do \autoref{alg:cpsi})]{%
    \includegraphics[width=0.45\linewidth]{\boxplotPath exp-0.5-500-2000-300-100/pKout-exp-0.5-500-2000-300-100.png}}
  \caption{\textit{Box plots} dos resultados dos algoritmos testados para 50 inicializações. Correlação exponencial, $\rho=0.5,\ n=500,\ p=2000,\ k^\dagger=100,\ \text{SNR}=300$.}\label{p3}
\end{figure}

Para o próximo experimento, mantivemos os parâmetros anteriores e trocando o tipo de correlação por constante, um cenário significativamente mais difícil. A \autoref{p4} apresenta resultados bastante drásticos em favor de estratégias NSPG com busca combinatorial. Em comparação com o CD, o ganho em iterações (tanto internas quanto externas) é substancial. Já em comparação com o NSPG sem busca combinatorial, a qualidade das soluções é muito superior. Entre o NSPG puro e o NSPG+PGCCD, esse primeiro se provou quase tão bom em qualidade de soluções, mas com mais iterações externas, as quais são a parte mais expressiva no tempo de execução total. 

\begin{figure}[H]
  \centering
  \subbottom[Valor objetivo alcançado.]{%
    \includegraphics[width=0.45\linewidth]{\boxplotPath con-0.5-500-2000-300-100/pF-con-0.5-500-2000-300-100.png}}
  \subbottom[Medida de recuperação do suporte.]{%
    \includegraphics[width=0.45\linewidth]{\boxplotPath con-0.5-500-2000-300-100/pSUP-con-0.5-500-2000-300-100.png}}\\
  \subbottom[Número de iterações (cada passagem pelas $p$ coordenadas contam como uma iteração do CD)]{%
    \includegraphics[width=0.45\linewidth]{\boxplotPath con-0.5-500-2000-300-100/pK-con-0.5-500-2000-300-100.png}}
  \subbottom[Número de iterações externas (iterações do \autoref{alg:cpsi})]{%
    \includegraphics[width=0.45\linewidth]{\boxplotPath con-0.5-500-2000-300-100/pKout-con-0.5-500-2000-300-100.png}}
  \caption{\textit{Box plots} dos resultados dos algoritmos testados para 50 inicializações. Correlação constante, $\rho=0.5,\ n=500,\ p=2000,\ k^\dagger=100,\ \text{SNR}=300$.}\label{p4}
\end{figure}

Por fim, com correlação exponencial, dividimos todas as dimensões por 2, tomamos $k^\dagger=25$, e aumentamos a correlação para $\rho=0.9$ (\autoref{p5}). Todos os métodos com otimização combinatorial obtiveram 100\% de recuperação do suporte original, enquanto o NSPG sem essa estratégia não. O número de iterações (internas e externas) do PGCCD foi muito mais alto, indicando novamente que as soluções obtidas desse método exigem mais perturbações do suporte antes de estabilizar.

\begin{figure}[H]
  \centering
  \subbottom[Valor objetivo alcançado.]{%
    \includegraphics[width=0.45\linewidth]{\boxplotPath exp-0.9-250-1000-300-25/pF-exp-0.9-250-1000-300-25.png}}
  \subbottom[Medida de recuperação do suporte.]{%
    \includegraphics[width=0.45\linewidth]{\boxplotPath exp-0.9-250-1000-300-25/pSUP-exp-0.9-250-1000-300-25.png}}\\
  \subbottom[Número de iterações (cada passagem pelas $p$ coordenadas contam como uma iteração do CD)]{%
    \includegraphics[width=0.45\linewidth]{\boxplotPath exp-0.9-250-1000-300-25/pK-exp-0.9-250-1000-300-25.png}}
  \subbottom[Número de iterações externas (iterações do Algoritmo \ref{alg:cpsi})]{%
    \label{pKout_exp_0.9}%
    \includegraphics[width=0.45\linewidth]{\boxplotPath exp-0.9-250-1000-300-25/pKout-exp-0.9-250-1000-300-25.png}}
  \caption{\textit{Box plots} dos resultados dos algoritmos testados para 50 inicializações. Correlação exponencial, $\rho=0.9,\ n=250,\ p=1000,\ k^\dagger=25,\ \text{SNR}=300$.}\label{p5}
\end{figure}



\subsection{Estratégias de Validação Cruzada}
Para a seleção do parâmetro de regularização $\lambda$, implementamos e comparamos três estratégias de validação cruzada (CV).

Para métodos inicializados com passo espectral (como o NSPG), o cálculo de $\lambda$ em todas as estratégias foi ajustado para compensar a escala introduzida pelo passo $\gamma_{j,0}$. Para métodos que não utilizam o passo espectral inicial (como o PGCCD), assume-se $\gamma_{j,0}=1$.

Adicionalmente, implementamos uma etapa de \textit{Refinamento Final} pós-CV. Independente da estratégia de validação cruzada utilizada, o algoritmo retém o melhor $\lambda^*$ encontrado. Em seguida, compara-se o erro de validação (MSE) de duas soluções finais geradas com este $\lambda^*$:
\begin{itemize}
    \item \textbf{Candidato do Caminho (\textit{Path-based})}: A solução obtida seguindo o caminho de regularização (seja decrescente, crescente ou adaptativo).
    \item \textbf{Candidato do Zero (\textit{Zero-based})}: Uma solução obtida reinicializando o solver a partir do vetor nulo (\textit{cold-start}) diretamente em $\lambda^*$.
\end{itemize}
A solução final escolhida é aquela que apresenta o menor MSE de validação. Esta etapa é crucial pois o \textit{warm-start} pode, por vezes, prender o algoritmo em ótimos locais sub-ótimos herdados de $\lambda$ anteriores, enquanto o \textit{cold-start} pode encontrar uma bacia de atração melhor para o mesmo parâmetro de regularização.

O impacto deste refinamento é analisado detalhadamente no \autoref{chap:anexos_refinement}. As métricas apresentadas incluem: (i) \textit{Wins/Ties/Losses}: Classificação qualitativa da escolha pelo zero em relação à recuperação do suporte, calculada apenas quando o candidato do zero é escolhido (por menor erro de validação). \textit{Wins} indica que $J_{zero} > J_{path}$, \textit{Ties} que $J_{zero} = J_{path}$, e \textit{Losses} que $J_{zero} < J_{path}$; e (ii) \textit{Sim Improv} (\textit{Similarity Improvement}): Diferença média na similaridade do suporte ($J_{zero} - J_{path}$) nestes mesmos casos.

\subsubsection{Validação Cruzada Padrão (Decrescente)}
Esta estratégia segue a abordagem clássica de caminhos de regularização, similar ao proposto em \cite{fastselect}. O algoritmo inicia com um valor de $\lambda_0$ suficientemente grande para garantir que a solução inicial seja o vetor nulo (ou esparsa). Em seguida, $\lambda$ é reduzido monotonicamente. A re-inicialização (warm-start) é utilizada, onde a solução otimizada para $\lambda_j$ serve como ponto de partida para a iteração $j+1$ com $\lambda_{j+1} < \lambda_j$.
A atualização de $\lambda$ é dada por:
\[
\lambda_{j+1} = 0.9 \cdot \min\left(\lambda_j, \gamma_{j,0} \cdot \frac{\max_{l \in S^c} |\nabla f(x^{j,0})_l|^2}{2}\right),
\]
onde $\gamma_{j,0}$ é o tamanho de passo inicial da iteração $j$ e $S^c$ é o complemento do suporte de $x^{j,0}$. Esta fórmula busca o próximo valor que permitiria a entrada de uma nova variável no suporte. Para o valor inicial, utilizamos $\lambda_0 = 1.01 \cdot \gamma_{0,0} \cdot \frac{\max_l |[\nabla f(x^0)]_l|^2}{2}$, garantindo que o ponto de partida seja o vetor nulo.

\subsubsection{Validação Cruzada Inversa (Crescente)}
Inspirada pela necessidade de explorar regiões de soluções mais densas primeiramente, esta estratégia percorre o caminho na direção oposta. O processo inicia com um $\lambda_0 = 1.01^{-1} \cdot \gamma_{0,0} \cdot \frac{\min_l |[\nabla f(x^0)]_l|^2}{2}$ pequeno para gerar um suporte relativamente cheio. Para garantir um ponto de partida válido com suporte não nulo, o que ainda pode acontecer com correlação constante usando PGCCD, executa-se uma fase de \textit{warmup} onde $\lambda$ é decrescido geometricamente por $0.9$ até que $\|x^{j,1}\|_0 > 0$.
A partir deste ponto, $\lambda$ é incrementado monotonicamente:
\[
\lambda_{j+1} = 0.9^{-1} \cdot \max\left(\lambda_j, \frac{\min_{l \in S} |x^{j,0}_l - \gamma_{j,0} \nabla f(x^{j,0})_l|^2}{2\gamma_{j,0}}\right).
\]
Esta atualização visa encontrar o próximo valor que forçaria a remoção de uma variável do suporte. Esta estratégia tende a ser mais rápida em alguns cenários por iniciar já com o suporte correto ou próximo dele, evitando o custo de adicionar variáveis uma a uma.

\subsubsection{Validação Cruzada Adaptativa Inteligente (\textit{Smart Adaptive})}
Esta estratégia híbrida visa combinar a robustez da busca decrescente com a eficiência da busca crescente, mitigando o risco de convergir para mínimos locais ruins dependendo do ponto de partida.
O procedimento consiste em duas fases:
\begin{enumerate}
    \item \textbf{Sondagem (\textit{Probe Phase})}: Avalia-se o erro de validação (MSE) nos dois extremos possíveis do caminho ($\lambda_{low}$ da estratégia inversa e $\lambda_{high}$ da padrão). Notavelmente, o cálculo de $\lambda_{low}$ também emprega a fase de \textit{warmup} descrita na estratégia inversa, garantindo que a sondagem inferiror inicie de um suporte não nulo válido. A direção inicial de varredura (crescente ou decrescente) é escolhida baseada no extremo que apresentar menor erro de validação.
    \item \textbf{Varredura com Reversão}: Inicia-se a varredura na direção escolhida. A cada passo, o algoritmo verifica se uma mudança de direção resultaria em uma redução significativa no erro de validação (melhora $> 1\%$). Se tal melhora for detectada, o algoritmo permite uma única inversão de direção. Isso possibilita encontrar vales de mínimos locais que uma varredura monotônica rígida não conseguiria explorar em detalhes e, em certos casos, uma parada mais rápida do algoritmo.
\end{enumerate}
O critério de parada da estratégia adaptativa inclui: (i) alcançar o limite de $\lambda$ oposto ao de partida (definido por uma razão fixa do extremo não escolhido); (ii) detecção de oscilação (dupla inversão de direção); ou (iii) colapso do suporte para vazio (no caso de direção crescente) ou cheio (decrescente).

A escolha da estratégia de validação cruzada para cada algoritmo foi baseada nos resultados apresentados na \autoref{fig:cv_comp} e nas Figuras \ref{fig:cv_comp_app1}--\ref{fig:cv_comp_app3} do apêndice. A primeira linha de cada figura apresenta a medida de recuperação do suporte (Jaccard modificado\footnote{Utilizamos uma variação do índice de Jaccard dada por $J(S, S^\dagger) = |S \cap S^\dagger| / \max(|S|, |S^\dagger|)$, onde $S$ e $S^\dagger$ são os suportes estimado e verdadeiro, respectivamente. Diferente do Jaccard padrão, esta medida penaliza desbalanços de cardinalidade com base no maior conjunto.}), enquanto a segunda linha apresenta o tempo médio de execução por valor de $\lambda$ testado.

Observa-se que métodos baseados em NSSP (incluindo PGCCD) beneficiam-se significativamente de caminhos de regularização bem definidos. Para o NSPG, observa-se que a estratégia inversa é altamente competitiva e robusta. Já para o PGCCD, a estratégia padrão (\textit{Regular}) mostrou-se a mais estável para garantir boa recuperação em todos os cenários, enquanto a abordagem adaptativa (\textit{Smart Adaptive}) mostrou-se ideal para a combinação NSPG+PGCCD, equilibrando eficiência e evasão de mínimos locais inadequados. Assim, os resultados reportados nas seções anteriores utilizam CV Inversa para NSPG, CV Adaptativa para NSPG+PGCCD e CV Padrão para PGCCD.

\begin{figure}[H]
  \centering
  \subbottom[NSPG+CPSI(1): Similaridade.]{%
    \label{cv_spg}%
    \includegraphics[width=0.32\linewidth]{\cvPath SPG/exp-0.5-2000/Comp_Sim-NSPG+CPSI1_Comparison_exp-0.5-2000.png}}
  \subbottom[L0Learn+CPSI(1) val $\lambda$ cv: Similaridade.]{%
    \label{cv_cdss}%
    \includegraphics[width=0.32\linewidth]{\cvPath L0LearnPSI1/exp-0.5-2000/Comp_Sim-L0LearnCPSI1_Comparison_exp-0.5-2000.png}}
  \subbottom[NSPG+PGCCD+CPSI(1): Similaridade.]{%
    \label{cv_spgpcdss}%
    \includegraphics[width=0.32\linewidth]{\cvPath SPGpCDSS/exp-0.5-2000/Comp_Sim-NSPG+PGCCD+CPSI1_Comparison_exp-0.5-2000.png}}\\
  \subbottom[NSPG+CPSI(1): Tempo.]{%
    \label{cv_spg_time}%
    \includegraphics[width=0.32\linewidth]{\cvPath SPG/exp-0.5-2000/Comp_Time-NSPG+CPSI1_Comparison_exp-0.5-2000.png}}
  \subbottom[L0Learn+CPSI(1) val $\lambda$ cv: Tempo.]{%
    \label{cv_cdss_time}%
    \includegraphics[width=0.32\linewidth]{\cvPath L0LearnPSI1/exp-0.5-2000/Comp_Time-L0LearnCPSI1_Comparison_exp-0.5-2000.png}}
  \subbottom[NSPG+PGCCD+CPSI(1): Tempo.]{%
    \label{cv_spgpcdss_time}%
    \includegraphics[width=0.32\linewidth]{\cvPath SPGpCDSS/exp-0.5-2000/Comp_Time-NSPG+PGCCD+CPSI1_Comparison_exp-0.5-2000.png}}
  \caption{Comparação das estratégias de validação cruzada para NSPG, PGCCD e NSPG+PGCCD (Correlação exponencial, $\rho=0.5,\ p=2000,\ k^\dagger=100,\ \text{SNR}=10$).}
  \label{fig:cv_comp}
\end{figure}


\subsection{Comparação com L0Learn}
Para contextualizar o desempenho do PGCCD em relação ao estado da arte, realizamos uma comparação com o pacote \texttt{L0Learn} \cite{l0learn} (versão de desenvolvimento do repositório \texttt{git} \url{https://github.com/fbkns/L0Learn}). O \texttt{L0Learn} é altamente otimizado e implementa algoritmos de descida coordenada com estratégias sofisticadas de \textit{active set}. 

Nesta comparação, distinguimos três variantes:
(i) \textbf{\texttt{L0Learn+CPSI1 val}}: utiliza o algoritmo nativo \texttt{CDPSI} da versão Git;
(ii) \textbf{\texttt{L0Learn val (CD only)}}: utiliza o algoritmo nativo \texttt{CD};
(iii) \textbf{\texttt{L0Learn+CPSI1 val path}}: utiliza o motor \texttt{CD} do \texttt{L0Learn} como base em um \textit{wrapper} \texttt{Julia} que permite chamadas para um par específico $(\lambda, x)$, aplicando nossa própria estratégia de validação cruzada padrão.
Cabe destacar que as versões nativas do \texttt{L0Learn} no repositório Git operam computando o caminho de regularização completo para um \textit{grid} de $\lambda$'s, apesar de o artigo original sugerir uma abordagem de caminho flexível. Assim, para as variantes (i) e (ii), recebemos o conjunto completo de soluções gerado pelo pacote e selecionamos a melhor baseada no erro do conjunto de validação. Já na variante (iii), o solver é invocado iterativamente seguindo nossa lógica de busca, garantindo consistência com o processo de refinamento local.

A \autoref{fig:l0learn_comp} apresenta os resultados para o cenário de correlação exponencial com $\rho=0.5$ e $p=2000$. Neste cenário, todos os métodos baseados em NSSP atingem recuperação quase perfeita do suporte a partir de $n \approx 800$. A combinação NSPG+PGCCD+CPSI1 atinge similaridade 1.0 ligeiramente antes dos demais, porém ao custo de um tempo de execução significativamente superior, crescendo de forma acentuada com $n$ (atingindo $\sim$245s para $n=2000$) --- enquanto os demais métodos permanecem abaixo de 15s. Esse cenário ilustra o principal compromisso da abordagem combinada: a otimização coordenada do PGCCD melhora a qualidade do suporte, mas a um custo computacional que escala supralinearmente. Quanto ao erro $L_\infty$ nos parâmetros, os métodos NSPG+CPSI1 e NSPG+PGCCD+CPSI1 apresentam os menores valores em todo o intervalo de $n$, indicando que a melhor recuperação do suporte traduz-se diretamente em melhor estimação dos parâmetros. O método \texttt{L0Learn val (CD only)} é consistentemente o pior tanto em similaridade quanto em $L_\infty$.

\begin{figure}[H]
  \centering
  \subbottom[Similaridade do Suporte.]{%
    \label{l0_sup}%
    \includegraphics[width=0.45\linewidth]{\mixedPath exp-0.5-2000/pnSim-NewTerminal_mixed_v2_exp_0.5_p2000.png}}
  \subbottom[Tempo de execução (s).]{%
    \label{l0_time}%
    \includegraphics[width=0.45\linewidth]{\mixedPath exp-0.5-2000/pnTime-NewTerminal_mixed_v2_exp_0.5_p2000.png}}\\
  \subbottom[Erro Máximo nos Parâmetros ($L_\infty$).]{%
    \label{l0_inf}%
    \includegraphics[width=0.45\linewidth]{\mixedPath exp-0.5-2000/pnInf-NewTerminal_mixed_v2_exp_0.5_p2000.png}}
  \caption{Comparação entre os métodos propostos e variantes do L0Learn. Correlação exponencial, $\rho=0.5,\ p=2000,\ k^\dagger=100,\ \text{SNR}=10$.}
  \label{fig:l0learn_comp}
\end{figure}

A vantagem do PGCCD torna-se mais pronunciada nos cenários de correlação constante, como ilustrado na \autoref{fig:l0learn_const}. Neste cenário com $\rho=0.9$ e $p=1000$, a alta correlação dificulta a seleção de variáveis para todos os métodos, porém tanto o NSPG+CPSI1 quanto o NSPG+PGCCD+CPSI1 apresentam superioridade consistente. O NSPG+PGCCD+CPSI1 atinge similaridade $\sim$0.95 já em $n \approx 250$, enquanto as variantes nativas do \texttt{L0Learn} necessitam de $n \geq 400$ para atingir desempenho comparável. O \texttt{L0Learn val (CD only)}, em particular, apresenta degradação significativa neste cenário de alta correlação. Em termos de erro $L_\infty$, os métodos NSSP mantêm vantagem em toda a faixa de $n$, convergindo para $\sim$0.1 a partir de $n \approx 800$.

\begin{figure}[H]
  \centering
  \subbottom[Similaridade do Suporte.]{%
    \label{l0_const_sup}%
    \includegraphics[width=0.45\linewidth]{\mixedPath const-0.9-1000/pnSim-NewTerminal_mixed_v2_const_0.9_p1000.png}}
  \subbottom[Tempo de execução (s).]{%
    \label{l0_const_time}%
    \includegraphics[width=0.45\linewidth]{\mixedPath const-0.9-1000/pnTime-NewTerminal_mixed_v2_const_0.9_p1000.png}}\\
  \subbottom[Erro Máximo nos Parâmetros ($L_\infty$).]{%
    \label{l0_const_inf}%
    \includegraphics[width=0.45\linewidth]{\mixedPath const-0.9-1000/pnInf-NewTerminal_mixed_v2_const_0.9_p1000.png}}
  \caption{Comparação entre os métodos propostos e variantes do L0Learn. Correlação constante, $\rho=0.9,\ p=1000,\ k^\dagger=20,\ \text{SNR}=5$.}
  \label{fig:l0learn_const}
\end{figure}

Essa diferença substancial nos cenários de correlação constante ocorre porque a descida coordenada do PGCCD é particularmente eficaz quando as correlações entre variáveis são uniformes, permitindo uma exploração mais eficiente do espaço de soluções. O mesmo padrão é observado na \autoref{fig:l0learn_anexo_1} ($\rho=0.5,\ p=2000$), onde o NSPG+PGCCD+CPSI1 alcança similaridade próxima a 1.0 já em $n \approx 400$ com custo temporal moderado ($\sim$22s para $n=2000$). No cenário de correlação exponencial com $\rho=0.9$ (\autoref{fig:l0learn_anexo_3}), as diferenças entre os métodos são menores e a convergência é mais uniforme, sugerindo que a estrutura exponencial decrescente das correlações facilita a separação das variáveis relevantes.

Em síntese, os resultados indicam que: (i) a combinação NSPG+PGCCD oferece a recuperação de suporte mais robusta, especialmente em cenários de correlação constante, ao custo de maior tempo de execução; (ii) o NSPG isolado é altamente competitivo em cenários de correlação exponencial, onde alcança excelente recuperação com custo computacional reduzido; (iii) a variante \texttt{L0Learn+CPSI1 val path}, que aplica nosso procedimento de validação cruzada ao motor do \texttt{L0Learn}, apresenta desempenho significativamente superior às variantes nativas, evidenciando a importância da estratégia de busca; e (iv) o erro $L_\infty$ confirma consistentemente que a qualidade da recuperação do suporte se traduz em melhor estimação dos parâmetros verdadeiros.
